# Image Segmentation

This repository contains the implementation of an multi class image segmentation pipeline for the COCO dataset. The pipeline includes dataset preparation (Task 1) and training a DeepLabV3 model with a MobileNetV3 backbone for multi-class segmentation (Task 2). The project meets all requirements, including a public Weights & Biases (WandB) dashboard, model weights, and inference examples.

## Repository Structure
- `process_data.py`: Script to filter COCO images and annotations for selected classes and generate multi-label segmentation masks.
- `split_data.py`: Script to split the dataset into training, validation, and test sets (8:1:1 ratio).
- `train.ipynb`: Script to train the model and test.
- `models`: Trained model weights.
- `coco_val_subset`: Directory for processed images and masks (not included in repo due to size; generated by `process_data.py`).
- `pyproject.toml`: Dependency configuration for `uv`.

## Prerequisites
- **uv**: Python environment and package manager ([installation instructions](https://github.com/astral-sh/uv#installation))
- **Dependencies**: Managed via `uv` using `pyproject.toml`.

- **COCO Dataset**:
  - Images: [http://images.cocodataset.org/zips/val2014.zip](http://images.cocodataset.org/zips/val2014.zip)
  - Annotations: [http://images.cocodataset.org/annotations/annotations_trainval2014.zip](http://images.cocodataset.org/annotations/annotations_trainval2014.zip)

## Setup Instructions
1. **Clone the Repository**:
   ```bash
   git clone <repository_url>
   cd <repository_name>
   ```

2. **Install uv**:
   - Follow the official instructions to install `uv`: [https://github.com/astral-sh/uv#installation](https://github.com/astral-sh/uv#installation).
   - Example (Linux):
     ```bash
     curl -LsSf https://astral.sh/uv/install.sh | sh
     ```

3. **Set Up Python Environment with uv**:
   - Create and activate a virtual environment, then install dependencies from `pyproject.toml`:
     ```bash
     uv venv
     source .venv/bin/activate
     uv sync
     ```

4. **Download and Extract COCO Dataset**:
   - Download the images and annotations from the links above.
   - Extract `val2014.zip` to a directory named `val2014/`.
   - Extract `annotations_trainval2014.zip` to access `instances_val2014.json`.

5. **Set Up WandB**:
   - Create a WandB account and log in:
     ```bash
     wandb login
     ```
   - Ensure your WandB API key is configured for logging metrics.

## Task 1: Dataset Preparation
Run `process_data.py` to filter the COCO dataset for the following classes:
- 1: Person
- 2: Bicycle
- 3: Car
- 4: Motorcycle
- 5: Airplane

The script generates multi-label segmentation masks (values 0â€“5 for background to airplane) and saves them with images in `coco_val/`. It processes 8,000 relevant images.

**Command**:
```bash
python process_data.py
```

**Edge Cases Handled**:
1. **Mask Overlap Handling**: When multiple class annotations overlap for a pixel, the last label processed is assigned (last-label-wins strategy) for simplicity, as overlaps were rare in the filtered dataset.
2. **Incorrect/Corrupted Annotations**: Annotations with invalid polygon coordinates (e.g., negative or out-of-bounds) were skipped, with a warning logged to ensure robustness.
3. **Excluding Crowded Annotations**: Images with more than 10 instances of the selected classes were excluded to avoid overly complex masks that could confuse the model.
4. **Images with No Valid Annotations**: Images lacking annotations for the five selected classes were filtered out during processing to maintain dataset relevance.

Run `split_data.py` to split the dataset into training (6,400 images), validation (800 images), and test (800 images) sets in an 8:1:1 ratio.

**Command**:
```bash
python split_data.py
```

## Task 2: Model Training
Train the DeepLabV3 model with MobileNetV3 backbone using `train.ipynb` notebook.

## Results
- **Test Metrics**:
The training metrics can be found here: [wandb](https://wandb.ai/ksrmanikumar-indian-institute-of-science/image_segmentation?nw=nwuserksrmanikumar)

- **Test Metrics**:
  - Overall IoU: 0.524
  - Class-wise IoU: Background (0.91), Person (0.70), Bicycle (0.26), Car (0.41), Motorcycle (0.42), Airplane (0.43)
- **Report**: See `report.pdf` for detailed modeling decisions, issue resolution, and computational resource summary.